{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a1ea17-223e-4130-af61-f49058fce71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/9bd7767f-e2b1-4f3e-a47c-\n",
      "[nltk_data]     80a0c1669bd6/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/9bd7767f-e2b1-4f3e-a47c-\n",
      "[nltk_data]     80a0c1669bd6/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/9bd7767f-e2b1-4f3e-a47c-\n",
      "[nltk_data]     80a0c1669bd6/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/9bd7767f-e2b1-4f3e-a47c-\n",
      "[nltk_data]     80a0c1669bd6/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/9bd7767f-e2b1-4f3e-a47c-\n",
      "[nltk_data]     80a0c1669bd6/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x92 in position 662: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 201\u001b[39m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReview.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m# Apply preprocessing pipeline\u001b[39;00m\n\u001b[32m    204\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mprocessed\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mReview\u001b[39m\u001b[33m\"\u001b[39m].apply(preprocess_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/codecs.py:322\u001b[39m, in \u001b[36mBufferedIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    320\u001b[39m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[32m    321\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.buffer + \u001b[38;5;28minput\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     (result, consumed) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m.buffer = data[consumed:]\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0x92 in position 662: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize tools\n",
    "spell = Speller(lang='en')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Dictionary of slang words and their replacements\n",
    "slang_dict = {\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"ikr\": \"I know right\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"bff\": \"best friend forever\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"hmu\": \"hit me up\",\n",
    "    \"rofl\": \"rolling on the floor laughing\"\n",
    "}\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions_dict = {\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"why's\": \"why is\"\n",
    "}\n",
    "\n",
    "# Remove any URLs that start with \"http\" or \"www\" from the text\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "# Extracts only the text, removing all HTML tags\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# Replace emoji with ''\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "# Replace internet slang/chat words\n",
    "def replace_slang(text):\n",
    "    escaped_slang_words = []\n",
    "    for word in slang_dict.keys():\n",
    "        escaped_word = re.escape(word)\n",
    "        escaped_slang_words.append(escaped_word)\n",
    "    slang_pattern = r'\\b(' + '|'.join(escaped_slang_words) + r')\\b'\n",
    "    def replace_match(match):\n",
    "        slang_word = match.group(0)\n",
    "        return slang_dict[slang_word.lower()]\n",
    "    replaced_text = re.sub(slang_pattern, replace_match, text, flags=re.IGNORECASE)\n",
    "    return replaced_text\n",
    "\n",
    "# Build the regex pattern for contractions\n",
    "escaped_contractions = []\n",
    "for contraction in contractions_dict.keys():\n",
    "    escaped_contraction = re.escape(contraction)\n",
    "    escaped_contractions.append(escaped_contraction)\n",
    "joined_contractions = \"|\".join(escaped_contractions)\n",
    "contractions_pattern = r'\\b(' + joined_contractions + r')\\b'\n",
    "compiled_pattern = re.compile(contractions_pattern, flags=re.IGNORECASE)\n",
    "\n",
    "# Function to replace contractions\n",
    "def replace_contractions(text):\n",
    "    def replace_match(match):\n",
    "        matched_word = match.group(0)\n",
    "        lower_matched_word = matched_word.lower()\n",
    "        expanded_form = contractions_dict[lower_matched_word]\n",
    "        return expanded_form\n",
    "    expanded_text = compiled_pattern.sub(replace_match, text)\n",
    "    return expanded_text\n",
    "\n",
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to remove numbers\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Function to correct spelling\n",
    "def correct_spelling(text):\n",
    "    return spell(text)\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Function to map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Function to lemmatize text with POS tagging\n",
    "def lemmatize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Function to apply all preprocessing steps\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()                  # Step 1: Lowercasing\n",
    "    text = remove_urls(text)             # Step 2: Remove URLs\n",
    "    text = remove_html(text)             # Step 3: Remove HTML tags\n",
    "    text = remove_emojis(text)           # Step 4: Remove Emojis\n",
    "    text = replace_slang(text)           # Step 5: Replace Slang\n",
    "    text = replace_contractions(text)    # Step 6: Expand Contractions\n",
    "    text = remove_punctuation(text)      # Step 7: Remove Punctuation\n",
    "    text = remove_numbers(text)          # Step 8: Remove Numbers\n",
    "    text = correct_spelling(text)        # Step 9: Correct Spelling\n",
    "    text = remove_stopwords(text)        # Step 10: Remove Stopwords\n",
    "    text = lemmatize_text(text)          # Step 11: Lemmatization\n",
    "    text = tokenize_text(text)           # Step 12: Tokenization\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Review.csv\")\n",
    "\n",
    "# Apply preprocessing pipeline\n",
    "df[\"processed\"] = df[\"Review\"].apply(preprocess_text)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv(\"Processed_Reviews2.csv\", index=False)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df[[\"Review\", \"processed\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c44573d-ad3f-4e29-a20f-5b40d60ac382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/9bd7767f-e2b1-4f3e-a47c-\n",
      "[nltk_data]     80a0c1669bd6/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/9bd7767f-e2b1-4f3e-a47c-\n",
      "[nltk_data]     80a0c1669bd6/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/9bd7767f-e2b1-4f3e-a47c-\n",
      "[nltk_data]     80a0c1669bd6/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/9bd7767f-e2b1-4f3e-a47c-\n",
      "[nltk_data]     80a0c1669bd6/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/9bd7767f-e2b1-4f3e-a47c-\n",
      "[nltk_data]     80a0c1669bd6/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0  The product arrived on time. Packaging was gre...   \n",
      "1           THIS PRODUCT IS JUST AMAZING! I LOVE IT.   \n",
      "2  I bought this phone for $799, and it has a 120...   \n",
      "3  Wow!!! This product is awesome... but a bit ex...   \n",
      "4                The laptop works perfectly fine.      \n",
      "\n",
      "                                           processed  \n",
      "0  [product, arrive, time, packaging, great, qual...  \n",
      "1                             [product, amaze, love]  \n",
      "2          [buy, phone, hz, display, totally, worth]  \n",
      "3            [wow, product, awesome, bit, expensive]  \n",
      "4                    [laptop, work, perfectly, fine]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize tools\n",
    "spell = Speller(lang='en')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Slang dictionary\n",
    "slang_dict = {\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"ikr\": \"I know right\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"bff\": \"best friend forever\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"hmu\": \"hit me up\",\n",
    "    \"rofl\": \"rolling on the floor laughing\"\n",
    "}\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions_dict = {\n",
    "    \"wasn't\": \"was not\", \"isn't\": \"is not\", \"aren't\": \"are not\",\n",
    "    \"weren't\": \"were not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n",
    "    \"shouldn't\": \"should not\", \"wouldn't\": \"would not\", \"won't\": \"will not\",\n",
    "    \"haven't\": \"have not\", \"hasn't\": \"has not\", \"hadn't\": \"had not\",\n",
    "    \"i'm\": \"i am\", \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\", \"we're\": \"we are\", \"they're\": \"they are\",\n",
    "    \"i've\": \"i have\", \"you've\": \"you have\", \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\", \"i'd\": \"i would\", \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\", \"she'd\": \"she would\", \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\", \"i'll\": \"i will\", \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\", \"she'll\": \"she will\", \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\", \"let's\": \"let us\", \"that's\": \"that is\",\n",
    "    \"who's\": \"who is\", \"what's\": \"what is\", \"where's\": \"where is\",\n",
    "    \"when's\": \"when is\", \"why's\": \"why is\"\n",
    "}\n",
    "\n",
    "# Build contractions pattern\n",
    "escaped_contractions = [re.escape(c) for c in contractions_dict.keys()]\n",
    "compiled_pattern = re.compile(r'\\b(' + '|'.join(escaped_contractions) + r')\\b', flags=re.IGNORECASE)\n",
    "\n",
    "# Preprocessing functions\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "def replace_slang(text):\n",
    "    escaped = [re.escape(w) for w in slang_dict.keys()]\n",
    "    pattern = r'\\b(' + '|'.join(escaped) + r')\\b'\n",
    "    return re.sub(pattern, lambda m: slang_dict[m.group(0).lower()], text, flags=re.IGNORECASE)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    return compiled_pattern.sub(lambda m: contractions_dict[m.group(0).lower()], text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def correct_spelling(text):\n",
    "    return spell(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([word for word in words if word.lower() not in stop_words])\n",
    "\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'): return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'): return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'): return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'): return wordnet.ADV\n",
    "    else: return wordnet.NOUN\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "    return \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags])\n",
    "\n",
    "def tokenize_text(text):\n",
    "    if not isinstance(text, str): return []\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Full preprocessing pipeline\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = replace_slang(text)\n",
    "    text = replace_contractions(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = correct_spelling(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_text(text)\n",
    "    text = tokenize_text(text)\n",
    "    return text\n",
    "\n",
    "# Load dataset with latin-1 encoding to handle special characters\n",
    "df = pd.read_csv(\"Review.csv\", encoding='latin-1')\n",
    "\n",
    "# Apply preprocessing pipeline\n",
    "df[\"processed\"] = df[\"Review\"].apply(preprocess_text)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv(\"Processed_Reviews2.csv\", index=False)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df[[\"Review\", \"processed\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84d748-6eb9-4e17-b867-f1d5eb22ad44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2025.12-py312",
   "language": "python",
   "name": "conda-env-anaconda-2025.12-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
