{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bce238a-7d52-49fd-b368-9964e2701c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/9bd7767f-e2b1-4f3e-a47c-\n",
      "[nltk_data]     80a0c1669bd6/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/9bd7767f-e2b1-4f3e-a47c-\n",
      "[nltk_data]     80a0c1669bd6/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/9bd7767f-e2b1-4f3e-a47c-\n",
      "[nltk_data]     80a0c1669bd6/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/9bd7767f-e2b1-4f3e-a47c-\n",
      "[nltk_data]     80a0c1669bd6/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/9bd7767f-e2b1-4f3e-a47c-\n",
      "[nltk_data]     80a0c1669bd6/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "                     Timestamp  \\\n",
      "0  2025/02/10 7:40:54 pm GMT+8   \n",
      "1  2025/02/10 7:41:00 pm GMT+8   \n",
      "2  2025/02/10 7:41:19 pm GMT+8   \n",
      "3  2025/02/10 7:46:40 pm GMT+8   \n",
      "4  2025/02/10 7:46:43 pm GMT+8   \n",
      "\n",
      "                                              Review  \n",
      "0  Im happy with uniten actually, even the people...  \n",
      "1  I’m having a pretty good time here, happy to m...  \n",
      "2        a very neutral place in terms of everything  \n",
      "3  I would say Uniten it's  a good university  bu...  \n",
      "4   UNITEN is well-regarded, particularly for its...  \n",
      "\n",
      "Column names: ['Timestamp', 'Review']\n",
      "\n",
      "Data types: Timestamp    object\n",
      "Review       object\n",
      "dtype: object\n",
      "\n",
      "Missing values: Timestamp    0\n",
      "Review       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize tools\n",
    "spell = Speller(lang='en')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"UNITENReview.csv\")\n",
    "\n",
    "# Display original data to identify issues\n",
    "print(\"Original Data:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "print(\"\\nData types:\", df.dtypes)\n",
    "print(\"\\nMissing values:\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5080d40-447e-4428-a09a-632bea01cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Processed data saved to Processed_UNITENReview.csv\n",
      "                                              Review  \\\n",
      "0  Im happy with uniten actually, even the people...   \n",
      "1  I’m having a pretty good time here, happy to m...   \n",
      "2        a very neutral place in terms of everything   \n",
      "3  I would say Uniten it's  a good university  bu...   \n",
      "4   UNITEN is well-regarded, particularly for its...   \n",
      "\n",
      "                                           processed  \n",
      "0      [im, happy, unite, actually, even, people, w]  \n",
      "1  [i, ’, m, pretty, good, time, happy, meet, w, ...  \n",
      "2                 [neutral, place, term, everything]  \n",
      "3  [would, say, united, good, university, issue, ...  \n",
      "4  [united, wellregarded, particularly, strong, e...  \n"
     ]
    }
   ],
   "source": [
    "# Slang dictionary\n",
    "slang_dict = {\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"ikr\": \"I know right\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"bff\": \"best friend forever\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"hmu\": \"hit me up\",\n",
    "    \"rofl\": \"rolling on the floor laughing\"\n",
    "}\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions_dict = {\n",
    "    \"wasn't\": \"was not\", \"isn't\": \"is not\", \"aren't\": \"are not\",\n",
    "    \"weren't\": \"were not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n",
    "    \"shouldn't\": \"should not\", \"wouldn't\": \"would not\", \"won't\": \"will not\",\n",
    "    \"haven't\": \"have not\", \"hasn't\": \"has not\", \"hadn't\": \"had not\",\n",
    "    \"i'm\": \"i am\", \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\", \"we're\": \"we are\", \"they're\": \"they are\",\n",
    "    \"i've\": \"i have\", \"you've\": \"you have\", \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\", \"i'd\": \"i would\", \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\", \"she'd\": \"she would\", \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\", \"i'll\": \"i will\", \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\", \"she'll\": \"she will\", \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\", \"let's\": \"let us\", \"that's\": \"that is\",\n",
    "    \"who's\": \"who is\", \"what's\": \"what is\", \"where's\": \"where is\",\n",
    "    \"when's\": \"when is\", \"why's\": \"why is\"\n",
    "}\n",
    "\n",
    "# Build contractions pattern\n",
    "escaped_contractions = [re.escape(c) for c in contractions_dict.keys()]\n",
    "compiled_pattern = re.compile(r'\\b(' + '|'.join(escaped_contractions) + r')\\b', flags=re.IGNORECASE)\n",
    "\n",
    "# Preprocessing functions\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "def replace_slang(text):\n",
    "    escaped = [re.escape(w) for w in slang_dict.keys()]\n",
    "    pattern = r'\\b(' + '|'.join(escaped) + r')\\b'\n",
    "    return re.sub(pattern, lambda m: slang_dict[m.group(0).lower()], text, flags=re.IGNORECASE)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    return compiled_pattern.sub(lambda m: contractions_dict[m.group(0).lower()], text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def correct_spelling(text):\n",
    "    return spell(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([word for word in words if word.lower() not in stop_words])\n",
    "\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'): return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'): return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'): return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'): return wordnet.ADV\n",
    "    else: return wordnet.NOUN\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "    return \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags])\n",
    "\n",
    "def tokenize_text(text):\n",
    "    if not isinstance(text, str): return []\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Full preprocessing pipeline\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = replace_slang(text)\n",
    "    text = replace_contractions(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = correct_spelling(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_text(text)\n",
    "    text = tokenize_text(text)\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing - replace \"Review\" with your actual column name if different\n",
    "df[\"processed\"] = df[\"Review\"].apply(preprocess_text)\n",
    "\n",
    "# Save result\n",
    "df.to_csv(\"Processed_UNITENReview.csv\", index=False)\n",
    "\n",
    "print(\"Done! Processed data saved to Processed_UNITENReview.csv\")\n",
    "print(df[[\"Review\", \"processed\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21c584-3237-4471-bb62-974e425fea43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2025.12-py312",
   "language": "python",
   "name": "conda-env-anaconda-2025.12-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
