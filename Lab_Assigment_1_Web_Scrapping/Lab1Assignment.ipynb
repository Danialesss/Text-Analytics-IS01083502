{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9cf54f-a19c-4e50-b266-5e2c9892b47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in d:\\anaconda3\\lib\\site-packages (4.41.0)\n",
      "Requirement already satisfied: webdriver-manager in d:\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\anaconda3\\lib\\site-packages (4.13.5)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.14.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: certifi>=2026.1.4 in d:\\anaconda3\\lib\\site-packages (from selenium) (2026.1.4)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in d:\\anaconda3\\lib\\site-packages (from selenium) (0.33.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in d:\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in d:\\anaconda3\\lib\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.6.3 in d:\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.6.3->selenium) (2.6.3)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in d:\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in d:\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in d:\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in d:\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (3.11)\n",
      "Requirement already satisfied: outcome in d:\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in d:\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (2.0.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in d:\\anaconda3\\lib\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in d:\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.6.3->selenium) (1.7.1)\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.5)\n",
      "Requirement already satisfied: python-dotenv in d:\\anaconda3\\lib\\site-packages (from webdriver-manager) (1.1.0)\n",
      "Requirement already satisfied: packaging in d:\\anaconda3\\lib\\site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in d:\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.31.0->selenium) (2.23)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in d:\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.4.4)\n",
      "Downloading beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: beautifulsoup4\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.13.5\n",
      "    Uninstalling beautifulsoup4-4.13.5:\n",
      "      Successfully uninstalled beautifulsoup4-4.13.5\n",
      "Successfully installed beautifulsoup4-4.14.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade selenium webdriver-manager beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e57aa9cd-8e33-497e-994e-4ca5f3306c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Amazon Review Scraper (Selenium)\n",
      "Product ASIN: B09MQWWP87 | Pages: 5\n",
      "==================================================\n",
      "Setting up browser...\n",
      "Opening Amazon sign-in page...\n",
      "\n",
      "==================================================\n",
      "PLEASE LOG IN TO AMAZON IN THE CHROME WINDOW.\n",
      "The script will wait and continue automatically.\n",
      "==================================================\n",
      "\n",
      "Login successful! Starting to scrape reviews...\n",
      "\n",
      "Scraping page 1 of 5...\n",
      "  URL: https://www.amazon.com/product-reviews/B09MQWWP87/ref=cm_cr_arp_d_paging_btm_next_1?ie=UTF8&reviewerType=all_reviews&pageNumber=1\n",
      "  Extracted 8 reviews from page 1.\n",
      "  Total reviews collected so far: 8\n",
      "  Waiting 4.5 seconds...\n",
      "\n",
      "Scraping page 2 of 5...\n",
      "  URL: https://www.amazon.com/product-reviews/B09MQWWP87/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=2\n",
      "  Extracted 8 reviews from page 2.\n",
      "  Total reviews collected so far: 16\n",
      "  Waiting 5.2 seconds...\n",
      "\n",
      "Scraping page 3 of 5...\n",
      "  URL: https://www.amazon.com/product-reviews/B09MQWWP87/ref=cm_cr_arp_d_paging_btm_next_3?ie=UTF8&reviewerType=all_reviews&pageNumber=3\n",
      "  Extracted 8 reviews from page 3.\n",
      "  Total reviews collected so far: 24\n",
      "  Waiting 4.9 seconds...\n",
      "\n",
      "Scraping page 4 of 5...\n",
      "  URL: https://www.amazon.com/product-reviews/B09MQWWP87/ref=cm_cr_arp_d_paging_btm_next_4?ie=UTF8&reviewerType=all_reviews&pageNumber=4\n",
      "  Extracted 8 reviews from page 4.\n",
      "  Total reviews collected so far: 32\n",
      "  Waiting 5.4 seconds...\n",
      "\n",
      "Scraping page 5 of 5...\n",
      "  URL: https://www.amazon.com/product-reviews/B09MQWWP87/ref=cm_cr_arp_d_paging_btm_next_5?ie=UTF8&reviewerType=all_reviews&pageNumber=5\n",
      "  Extracted 8 reviews from page 5.\n",
      "  Total reviews collected so far: 40\n",
      "\n",
      "Browser closed.\n",
      "\n",
      "==================================================\n",
      "Scraping complete! Total reviews collected: 40\n",
      "\n",
      "Data saved to 'amazon_reviews.csv'\n",
      "Total records saved: 40\n",
      "\n",
      "Shape of dataset: (40, 3)\n",
      "Columns: ['reviewer_name', 'review_date', 'review_content']\n",
      "\n",
      "Null values:\n",
      "reviewer_name     0\n",
      "review_date       0\n",
      "review_content    0\n",
      "dtype: int64\n",
      "\n",
      "First 10 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amanda_D</td>\n",
       "      <td>February 19, 2026</td>\n",
       "      <td>Bought two of these for my kids awhile back. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chonnn</td>\n",
       "      <td>February 18, 2026</td>\n",
       "      <td>Love the color. Love the size. So many pockets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>February 22, 2026</td>\n",
       "      <td>I bought this bag last minute before my travel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sandra smith</td>\n",
       "      <td>February 7, 2026</td>\n",
       "      <td>This is an amazing bag for travel , work etc. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kritzia Lopez</td>\n",
       "      <td>February 25, 2026</td>\n",
       "      <td>Exactly as depicted in the image and descripti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M MADSON</td>\n",
       "      <td>January 5, 2026</td>\n",
       "      <td>This backpack/bag is huge! It has one pocket f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Astghik Ohanyan</td>\n",
       "      <td>January 12, 2026</td>\n",
       "      <td>I have used many travel backpacks designed for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>happykbd26</td>\n",
       "      <td>February 5, 2026</td>\n",
       "      <td>Absolutely a great buy!  Can fit so much while...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Amanda_D</td>\n",
       "      <td>February 19, 2026</td>\n",
       "      <td>Bought two of these for my kids awhile back. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chonnn</td>\n",
       "      <td>February 18, 2026</td>\n",
       "      <td>Love the color. Love the size. So many pockets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     reviewer_name        review_date  \\\n",
       "0         Amanda_D  February 19, 2026   \n",
       "1           Chonnn  February 18, 2026   \n",
       "2  Amazon Customer  February 22, 2026   \n",
       "3     sandra smith   February 7, 2026   \n",
       "4    Kritzia Lopez  February 25, 2026   \n",
       "5         M MADSON    January 5, 2026   \n",
       "6  Astghik Ohanyan   January 12, 2026   \n",
       "7       happykbd26   February 5, 2026   \n",
       "8         Amanda_D  February 19, 2026   \n",
       "9           Chonnn  February 18, 2026   \n",
       "\n",
       "                                      review_content  \n",
       "0  Bought two of these for my kids awhile back. T...  \n",
       "1  Love the color. Love the size. So many pockets...  \n",
       "2  I bought this bag last minute before my travel...  \n",
       "3  This is an amazing bag for travel , work etc. ...  \n",
       "4  Exactly as depicted in the image and descripti...  \n",
       "5  This backpack/bag is huge! It has one pocket f...  \n",
       "6  I have used many travel backpacks designed for...  \n",
       "7  Absolutely a great buy!  Can fit so much while...  \n",
       "8  Bought two of these for my kids awhile back. T...  \n",
       "9  Love the color. Love the size. So many pockets...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CISB5123 Text Analytics - Lab Assignment 1: Web Scraping\n",
    "# Name: Tun Danial Adli Bin Tun Ali\n",
    "# Student ID: IS01083502\n",
    "\n",
    "# Name: Amirul Irfaan Bin Mohd.Hishamuddin\n",
    "# Student ID: IS01083864\n",
    "\n",
    "from selenium import webdriver                                     # For automating browser interaction\n",
    "from selenium.webdriver.chrome.options import Options              # For configuring Chrome browser\n",
    "from selenium.webdriver.common.by import By                        # For locating HTML elements\n",
    "from selenium.webdriver.support.ui import WebDriverWait            # For waiting until elements load\n",
    "from selenium.webdriver.support import expected_conditions as EC   # For wait conditions\n",
    "from bs4 import BeautifulSoup                                      # For parsing HTML content\n",
    "import pandas as pd                                                # For data manipulation and CSV export\n",
    "import time                                                        # For adding delays between requests\n",
    "import random                                                      # For randomizing delay intervals\n",
    "\n",
    "# --- Configuration ---\n",
    "# The ASIN is extracted from the Amazon product URL (after /dp/)\n",
    "# Product: Backpack for Women, Carry-On Travel Backpack\n",
    "# URL: https://www.amazon.com/dp/B09MQWWP87\n",
    "\n",
    "ASIN = 'B09MQWWP87'   # Amazon product ASIN\n",
    "MAX_PAGES = 5          # Number of review pages to scrape (limited to 5)\n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Sets up and returns a Selenium Chrome WebDriver instance.\n",
    "    Runs with a visible browser window to avoid anti-bot detection.\n",
    "    \n",
    "    Returns:\n",
    "        webdriver.Chrome: Configured Chrome WebDriver instance\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--window-size=1920,1080')\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    # Hide automation detection from websites\n",
    "    driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "        'source': 'Object.defineProperty(navigator, \"webdriver\", {get: () => undefined})'\n",
    "    })\n",
    "    \n",
    "    return driver\n",
    "\n",
    "\n",
    "def get_review_url(asin, page_number):\n",
    "    \"\"\"\n",
    "    Constructs the URL for a specific page of Amazon product reviews.\n",
    "    \n",
    "    Parameters:\n",
    "        asin (str): The Amazon Standard Identification Number of the product\n",
    "        page_number (int): The page number of reviews to fetch\n",
    "    \n",
    "    Returns:\n",
    "        str: The complete URL for the review page\n",
    "    \"\"\"\n",
    "    url = (f'https://www.amazon.com/product-reviews/{asin}'\n",
    "           f'/ref=cm_cr_arp_d_paging_btm_next_{page_number}'\n",
    "           f'?ie=UTF8&reviewerType=all_reviews&pageNumber={page_number}')\n",
    "    return url\n",
    "\n",
    "\n",
    "def scroll_page(driver):\n",
    "    \"\"\"\n",
    "    Scrolls down the page gradually to trigger lazy-loading of content.\n",
    "    Amazon loads review content dynamically as the user scrolls.\n",
    "    \n",
    "    Parameters:\n",
    "        driver (webdriver.Chrome): The Selenium WebDriver instance\n",
    "    \"\"\"\n",
    "    for i in range(5):\n",
    "        driver.execute_script(f\"window.scrollTo(0, {500 + i * 500});\")\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "def extract_reviews(soup):\n",
    "    \"\"\"\n",
    "    Extracts reviewer name, date, and content from a parsed Amazon review page.\n",
    "    Uses the 'customer_review' ID prefix to find review containers,\n",
    "    'a-profile-name' class for reviewer names, 'review-date' data-hook for dates,\n",
    "    and 'review-text-content' class for review body text.\n",
    "    \n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML of the review page\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries with reviewer_name, review_date, review_content\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    \n",
    "    # Find all review containers\n",
    "    review_divs = soup.find_all('div', id=lambda x: x and x.startswith('customer_review'))\n",
    "    \n",
    "    for review_div in review_divs:\n",
    "        try:\n",
    "            # Extract reviewer name from the profile section\n",
    "            name_tag = review_div.find('span', class_='a-profile-name')\n",
    "            reviewer_name = name_tag.text.strip() if name_tag else 'Anonymous'\n",
    "            \n",
    "            # Extract review date\n",
    "            date_tag = review_div.find('span', {'data-hook': 'review-date'})\n",
    "            review_date = date_tag.text.strip() if date_tag else 'No date'\n",
    "            if ' on ' in review_date:\n",
    "                review_date = review_date.split(' on ')[-1]\n",
    "            \n",
    "            # Extract review content using the updated class name\n",
    "            body_tag = review_div.find('span', class_='review-text-content')\n",
    "            review_content = body_tag.text.strip() if body_tag else ''\n",
    "            \n",
    "            # Only include reviews that have actual text content\n",
    "            if review_content:\n",
    "                reviews.append({\n",
    "                    'reviewer_name': reviewer_name,\n",
    "                    'review_date': review_date,\n",
    "                    'review_content': review_content\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error extracting a review: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "\n",
    "def scrape_all_reviews(asin, max_pages=5):\n",
    "    \"\"\"\n",
    "    Main scraping function. Opens Amazon sign-in page for user to log in,\n",
    "    then navigates through review pages and extracts all review data.\n",
    "    \n",
    "    Parameters:\n",
    "        asin (str): The Amazon product ASIN\n",
    "        max_pages (int): Maximum number of pages to scrape (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        list: A combined list of all review dictionaries from all pages\n",
    "    \"\"\"\n",
    "    all_reviews = []\n",
    "    \n",
    "    print(\"Setting up browser...\")\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    try:\n",
    "        # Open Amazon sign-in page for user to log in\n",
    "        print(\"Opening Amazon sign-in page...\")\n",
    "        driver.get('https://www.amazon.com/ap/signin?openid.pape.max_auth_age=0'\n",
    "                    '&openid.return_to=https%3A%2F%2Fwww.amazon.com%2F'\n",
    "                    '&openid.identity=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select'\n",
    "                    '&openid.assoc_handle=usflex'\n",
    "                    '&openid.mode=checkid_setup'\n",
    "                    '&openid.claimed_id=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select'\n",
    "                    '&openid.ns=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0')\n",
    "        \n",
    "        # Wait for user to complete login\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"PLEASE LOG IN TO AMAZON IN THE CHROME WINDOW.\")\n",
    "        print(\"The script will wait and continue automatically.\")\n",
    "        print(\"=\" * 50 + \"\\n\")\n",
    "        \n",
    "        while True:\n",
    "            current_url = driver.current_url.lower()\n",
    "            page_title = driver.title.lower()\n",
    "            is_still_login = any(keyword in current_url for keyword in \n",
    "                                ['signin', 'captcha', 'validatecaptcha', 'ap/signin'])\n",
    "            is_still_login = is_still_login or any(keyword in page_title for keyword in \n",
    "                                ['sign-in', 'sign in', 'captcha', 'verification'])\n",
    "            if not is_still_login:\n",
    "                print(\"Login successful! Starting to scrape reviews...\\n\")\n",
    "                break\n",
    "            time.sleep(3)\n",
    "        \n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Scrape reviews page by page\n",
    "        for page in range(1, max_pages + 1):\n",
    "            print(f\"Scraping page {page} of {max_pages}...\")\n",
    "            \n",
    "            url = get_review_url(asin, page)\n",
    "            print(f\"  URL: {url}\")\n",
    "            \n",
    "            # Load the review page\n",
    "            driver.get(url)\n",
    "            time.sleep(4)\n",
    "            \n",
    "            # Scroll down to load all review content\n",
    "            scroll_page(driver)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Parse the page\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Extract reviews\n",
    "            page_reviews = extract_reviews(soup)\n",
    "            \n",
    "            if not page_reviews:\n",
    "                print(f\"  No reviews found on page {page}. Stopping.\")\n",
    "                break\n",
    "            \n",
    "            all_reviews.extend(page_reviews)\n",
    "            print(f\"  Extracted {len(page_reviews)} reviews from page {page}.\")\n",
    "            print(f\"  Total reviews collected so far: {len(all_reviews)}\")\n",
    "            \n",
    "            # Random delay between pages to avoid detection\n",
    "            if page < max_pages:\n",
    "                delay = random.uniform(3, 6)\n",
    "                print(f\"  Waiting {delay:.1f} seconds...\\n\")\n",
    "                time.sleep(delay)\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"\\nBrowser closed.\")\n",
    "    \n",
    "    return all_reviews\n",
    "\n",
    "\n",
    "def save_to_csv(reviews, filename='amazon_reviews.csv'):\n",
    "    \"\"\"\n",
    "    Saves the collected reviews to a CSV file using pandas.\n",
    "    \n",
    "    Parameters:\n",
    "        reviews (list): List of review dictionaries\n",
    "        filename (str): Output CSV filename (default: 'amazon_reviews.csv')\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The pandas DataFrame created from the reviews\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(reviews)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nData saved to '{filename}'\")\n",
    "    print(f\"Total records saved: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Execute Scraping ---\n",
    "print(\"=\" * 50)\n",
    "print(\"Amazon Review Scraper (Selenium)\")\n",
    "print(f\"Product ASIN: {ASIN} | Pages: {MAX_PAGES}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_reviews = scrape_all_reviews(ASIN, MAX_PAGES)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Scraping complete! Total reviews collected: {len(all_reviews)}\")\n",
    "\n",
    "# --- 5. Save to CSV ---\n",
    "df_reviews = save_to_csv(all_reviews, 'amazon_reviews.csv')\n",
    "\n",
    "# --- 6. Preview the Data ---\n",
    "print(f\"\\nShape of dataset: {df_reviews.shape}\")\n",
    "print(f\"Columns: {list(df_reviews.columns)}\")\n",
    "print(f\"\\nNull values:\\n{df_reviews.isnull().sum()}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "df_reviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a6d09-08de-4f3d-8271-83389d31a2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
